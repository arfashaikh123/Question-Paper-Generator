{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Smart Question Paper Generator (Advanced)\n",
                "\n",
                "This notebook generates a question paper by:\n",
                "1.  **OCR**: Using `nanonets/Nanonets-OCR2-3B` to extract text from previous years' papers.\n",
                "2.  **Syllabus Analysis**: Extracting chapter-wise hours from the syllabus.\n",
                "3.  **Weightage Calculation**: Combining syllabus hours and question frequency to prioritize topics.\n",
                "4.  **Generation**: Using Groq's LLM to create a new paper based on these weights."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install necessary libraries\n",
                "# Note: 'flash_attn' is recommended for faster processing if you have a compatible GPU\n",
                "!pip install PyPDF2 langchain langchain-groq python-dotenv transformers torch torchvision pillow accelerate bitsandbytes pdf2image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import re\n",
                "import getpass\n",
                "import torch\n",
                "from PIL import Image\n",
                "from pdf2image import convert_from_path\n",
                "from transformers import AutoModelForCausalLM, AutoProcessor\n",
                "from PyPDF2 import PdfReader\n",
                "from langchain_groq import ChatGroq\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "\n",
                "# Set up Groq API Key\n",
                "if \"GROQ_API_KEY\" not in os.environ:\n",
                "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
                "\n",
                "# Check for GPU\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. OCR with Nanonets (Local/Colab Model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Nanonets Model\n",
                "model_id = \"nanonets/Nanonets-OCR2-3B\"\n",
                "\n",
                "try:\n",
                "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        model_id,\n",
                "        trust_remote_code=True,\n",
                "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
                "        device_map=\"auto\"\n",
                "    )\n",
                "    print(\"Nanonets model loaded successfully.\")\n",
                "except Exception as e:\n",
                "    print(f\"Failed to load model: {e}\")\n",
                "    print(\"Ensure you have enough VRAM (approx 8GB+ for 3B model in fp16).\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def ocr_pdf(pdf_path):\n",
                "    \"\"\"Converts PDF to images and runs OCR on each page.\"\"\"\n",
                "    extracted_text = \"\"\n",
                "    try:\n",
                "        images = convert_from_path(pdf_path)\n",
                "        print(f\"Processing {len(images)} pages for {os.path.basename(pdf_path)}...\")\n",
                "        \n",
                "        for i, image in enumerate(images):\n",
                "            prompt = \"<|image|>Extract the text from this document accurately into markdown format.\"\n",
                "            inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
                "            \n",
                "            generated_ids = model.generate(\n",
                "                **inputs,\n",
                "                max_new_tokens=2048,\n",
                "                do_sample=False  # Deterministic output usually better for OCR\n",
                "            )\n",
                "            \n",
                "            generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
                "            # Setup to just get the assistant response if the model includes prompt in output\n",
                "            # (Adjust based on specific model behavior, some output prompt + completion)\n",
                "            \n",
                "            # Simple cleaning if prompt is repeated\n",
                "            if prompt in generated_text:\n",
                "                generated_text = generated_text.replace(prompt, \"\")\n",
                "            \n",
                "            extracted_text += f\"--- Page {i+1} --- \\n{generated_text}\\n\"\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"Error processing {pdf_path}: {e}\")\n",
                "    \n",
                "    return extracted_text"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Syllabus Parsing & Weightage Logic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SyllabusParser:\n",
                "    def __init__(self, text):\n",
                "        self.text = text\n",
                "        self.modules = {}\n",
                "\n",
                "    def parse_hours(self):\n",
                "        # Regex to find \"Module X ... Y Hours\" or similar patterns\n",
                "        # This needs to be adapted to the specific syllabus format\n",
                "        # Pattern: look for \"Module\" identifier, then capture the Topic Name, then find \"Hours\" or \"Hrs\"\n",
                "        \n",
                "        # Example Pattern:  \"1. Introduction ... 04 Hours\"\n",
                "        # We'll use a generic regex attempting to capture typical syllabus lines\n",
                "        \n",
                "        lines = self.text.split('\\n')\n",
                "        current_module = None\n",
                "        \n",
                "        for line in lines:\n",
                "            # Heuristic: Check for lines ending in Hours/Hrs\n",
                "            match = re.search(r'(\\d+)(?:\\s+)?(?:Hours|Hrs)', line, re.IGNORECASE)\n",
                "            if match:\n",
                "                hours = int(match.group(1))\n",
                "                # Try to extract module name from the start of the line\n",
                "                # Removing the hours part\n",
                "                topic_name = line[:match.start()].strip()\n",
                "                \n",
                "                # Further clean topic name (remove leading numbers/bullets)\n",
                "                topic_name = re.sub(r'^[\\d\\.\\W]+', '', topic_name).strip()\n",
                "                \n",
                "                if topic_name and len(topic_name) > 3: # valid topic\n",
                "                    self.modules[topic_name] = hours\n",
                "\n",
                "        return self.modules\n",
                "\n",
                "def calculate_weights(modules, previous_papers_text):\n",
                "    # 1. Syllabus Weight (Normalized Hours)\n",
                "    total_hours = sum(modules.values()) if modules else 1\n",
                "    syllabus_weights = {k: v/total_hours for k, v in modules.items()}\n",
                "    \n",
                "    # 2. Frequency Weight (This is hard to do perfectly without NLP, using simple keyword matching)\n",
                "    paper_text_lower = previous_papers_text.lower()\n",
                "    frequency_counts = {}\n",
                "    \n",
                "    for topic in modules.keys():\n",
                "        # Count occurrences of the topic or key terms in the previous papers\n",
                "        # Taking first 2 words of topic as keywords\n",
                "        keywords = topic.split()[:2]\n",
                "        kw_regex = r\"|\".join([re.escape(k.lower()) for k in keywords if len(k) > 3])\n",
                "        \n",
                "        if kw_regex:\n",
                "            count = len(re.findall(kw_regex, paper_text_lower))\n",
                "            frequency_counts[topic] = count\n",
                "        else:\n",
                "            frequency_counts[topic] = 0\n",
                "            \n",
                "    total_freq = sum(frequency_counts.values()) if frequency_counts else 1\n",
                "    freq_weights = {k: v/total_freq if total_freq > 0 else 0 for k, v in frequency_counts.items()}\n",
                "    \n",
                "    # 3. Combined Weight\n",
                "    final_weights = {}\n",
                "    for topic in modules:\n",
                "        # 50% Hours, 50% Frequency\n",
                "        final_weights[topic] = (syllabus_weights.get(topic, 0) * 0.5) + (freq_weights.get(topic, 0) * 0.5)\n",
                "        \n",
                "    return final_weights"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Execution Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Main execution flow\n",
                "qa_folder = \"QA\"\n",
                "syllabus_text = \"\"\n",
                "previous_papers_text = \"\"\n",
                "\n",
                "if os.path.exists(qa_folder):\n",
                "    for filename in os.listdir(qa_folder):\n",
                "        file_path = os.path.join(qa_folder, filename)\n",
                "        if filename.lower().endswith(\".pdf\"):\n",
                "            print(f\"Processing: {filename}\")\n",
                "            \n",
                "            # Determine if syllabus or paper (simple check)\n",
                "            is_syllabus = \"syllabus\" in filename.lower()\n",
                "            \n",
                "            # Use OCR for papers (better quality) or standard PDF read for syllabus if it's text-based\n",
                "            # Assuming we want high quality for everything, let's use OCR for papers\n",
                "            # For syllabus, often simple extraction works, but let's use OCR if we can afford the time\n",
                "            \n",
                "            if is_syllabus:\n",
                "                # Syllabus might be text-based PDF, try simple extraction first for speed\n",
                "                try:\n",
                "                    reader = PdfReader(file_path)\n",
                "                    text = \"\".join([p.extract_text() for p in reader.pages])\n",
                "                    syllabus_text += text\n",
                "                except:\n",
                "                    # Fallback to OCR\n",
                "                    syllabus_text += ocr_pdf(file_path)\n",
                "            else:\n",
                "                previous_papers_text += ocr_pdf(file_path)\n",
                "\n",
                "# Parse Syllabus\n",
                "parser = SyllabusParser(syllabus_text)\n",
                "modules = parser.parse_hours()\n",
                "print(\"Detected Modules & Hours:\", modules)\n",
                "\n",
                "# Calculate Weights\n",
                "weights = calculate_weights(modules, previous_papers_text)\n",
                "print(\"Calculated Topic Weights:\", weights)\n",
                "\n",
                "# Format High Importance Topics for Prompt\n",
                "sorted_topics = sorted(weights.items(), key=lambda x: x[1], reverse=True)\n",
                "top_focus_areas = \"\\n\".join([f\"- {t[0]} (Weight: {t[1]:.2f})\" for t in sorted_topics[:5]])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Generation with Groq"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "llm = ChatGroq(\n",
                "    temperature=0.4,\n",
                "    model_name=\"llama-3.3-70b-versatile\"\n",
                ")\n",
                "\n",
                "prompt_text = \"\"\"You are an expert question paper setter. Design a question paper that prioritizes the following High Importance Topics based on their syllabus weight and historical frequency:\n",
                "\n",
                "**High Priority Topics:**\n",
                "{top_focus_areas}\n",
                "\n",
                "**Syllabus Context:**\n",
                "{syllabus_snippet}\n",
                "\n",
                "**Instructions:**\n",
                "1. Allocate more marks/questions to the High Priority Topics listed above.\n",
                "2. Ensure the paper covers the entire syllabus but skews difficulty/volume towards the weighted topics.\n",
                "3. Follow the standard university pattern (e.g., Q1 Compulsory, Q2-Q6 with choices).\n",
                "4. Create valid, conceptual, and application-based questions.\n",
                "5. Output in clean Markdown.\n",
                "\"\"\"\n",
                "\n",
                "prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"You are a smart exam setter algorithm.\"),\n",
                "    (\"human\", prompt_text)\n",
                "])\n",
                "\n",
                "chain = prompt | llm\n",
                "\n",
                "try:\n",
                "    response = chain.invoke({\n",
                "        \"top_focus_areas\": top_focus_areas,\n",
                "        \"syllabus_snippet\": syllabus_text[:15000] # Truncate for context limit\n",
                "    })\n",
                "    \n",
                "    print(\"\\n=== Generated Question Paper ===\\n\")\n",
                "    final_paper = response.content\n",
                "    print(final_paper)\n",
                "    \n",
                "    with open(\"Smart_Generated_Paper.md\", \"w\", encoding=\"utf-8\") as f:\n",
                "        f.write(final_paper)\n",
                "        \n",
                "except Exception as e:\n",
                "    print(f\"Generation Failed: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}